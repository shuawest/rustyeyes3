So here is my thought. The model has no reference to translate the model coordinates to the screen coodinates. Perhaps I can make a mode of the application where I look at a position on the screen where my true mouse cursor is and click, or hit a specific key to take a screenshot paired with the mouse location. Then the results of the onnx and moondream2 result can be calibated against all the captured images. Design this approach and document a plan in the spec. Ask me any clarifying questions needed. Let's get the spec right before implementing.
